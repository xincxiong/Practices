{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- denoising autoencoder\n",
    "-  The encoder has two convolutional layers and two max pooling layers. \n",
    "-  Both Convolution layer-1 and Convolution layer-2 have 32-3 x 3 filters. There are two max-pooling layers each of size 2 x 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "with tf.name_scope('en-convolutions'):\n",
    " conv1 = tf.layers.conv2d(inputs_,filters=32,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv1')\n",
    "# Now 28x28x32\n",
    " \n",
    "with tf.name_scope('en-pooling'):\n",
    " maxpool1 = tf.layers.max_pooling2d(conv1,pool_size=(2,2),strides=(2,2),name='pool1')\n",
    "# Now 14x14x32\n",
    " \n",
    "with tf.name_scope('en-convolutions'):\n",
    " conv2 = tf.layers.conv2d(maxpool1,filters=32,kernel_size=(3,3),strides=(1,1),padding='SAME',use_bias=True,activation=lrelu,name='conv2')\n",
    "# Now 14x14x32\n",
    " \n",
    "with tf.name_scope('encoding'):\n",
    " encoded = tf.layers.max_pooling2d(conv2,pool_size=(2,2),strides=(2,2),name='encoding')\n",
    "# Now 7x7x32.\n",
    " \n",
    "#latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "with tf.name_scope('decoder'):\n",
    " conv3 = tf.layers.conv2d(encoded,filters=32,kernel_size=(3,3),strides=(1,1),name='conv3',padding='SAME',use_bias=True,activation=lrelu)\n",
    "#Now 7x7x32 \n",
    " \n",
    " upsample1 = tf.layers.conv2d_transpose(conv3,filters=32,kernel_size=3,padding='same',strides=2,name='upsample1')\n",
    "# Now 14x14x32\n",
    " \n",
    " upsample2 = tf.layers.conv2d_transpose(upsample1,filters=32,kernel_size=3,padding='same',strides=2,name='upsample2')\n",
    "# Now 28x28x32\n",
    " \n",
    " logits = tf.layers.conv2d(upsample2,filters=1,kernel_size=(3,3),strides=(1,1),name='logits',padding='SAME',use_bias=True)\n",
    "#Now 28x28x1\n",
    " \n",
    "# Pass logits through sigmoid to get denoisy image\n",
    " decoded = tf.sigmoid(logits,name='recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_X = mnist.train.images\n",
    "test_X = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis\n",
    "print('Training data shape' :train_X.shape)\n",
    "print('Testing data shape' :test_X.shape)\n",
    " \n",
    "nsample = 1\n",
    "rand_train_idx = np.random.randint(mnist.train.images.shape[0], size=nsample)\n",
    " \n",
    "for i in rand_train_idx:\n",
    "  curr_img = np.reshape(mnist.train.images[i, :], (28,28))\n",
    "  curr_lbl = np.argmax(mnist.train.labels[i, :])\n",
    "  plt.matshow(curr_img, cmap=plt.get_cmap('gray'))\n",
    "  plt.title(\"\"+str(i)+\"th Training Image \"+ \"(label: \" + str(curr_lbl) + \")\")\n",
    "  plt.show()\n",
    " \n",
    "rand_test_idx = np.random.randint(mnist.test.images.shape[0], size=nsample)\n",
    " \n",
    "for i in rand_test_idx:\n",
    "  curr_img = np.reshape(mnist.test.images[i, :], (28,28))\n",
    "  curr_lbl = np.argmax(mnist.test.labels[i, :])\n",
    "  plt.matshow(curr_img, cmap=plt.get_cmap('gray'))\n",
    "  plt.title(\"\"+str(i)+\"th Test Image \"+ \"(label: \" + str(curr_lbl) + \")\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_train_x = mnist.train.next_batch(batch_size)\n",
    "batch_test_x = mnist.test.next_batch(batch_size)\n",
    "imgs_train= batch_train_x[0].reshape((-1, 28, 28, 1))                               \n",
    "imgs_test = batch_test_x[0].reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "x_train_noisy = imgs_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=imgs_train.shape)\n",
    "x_test_noisy = imgs_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=imgs_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: x_train_noisy,targets_: imgs,learning_rate:lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_cost_test = sess.run(cost, feed_dict={inputs_: x_test_noisy,targets_: imgs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.append(batch_cost)\n",
    "valid_loss.append(batch_cost_test)\n",
    "plt.plot(range(e+1), loss, 'bo', label='Training loss')\n",
    "plt.plot(range(e+1), valid_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
